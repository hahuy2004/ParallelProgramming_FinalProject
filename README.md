# CIFAR-10 Autoencoder + SVM v·ªõi CUDA

**ƒê·ªì √°n cu·ªëi k·ª≥ - L·∫≠p tr√¨nh Song song (CSC14120)**

## üìã T·ªïng quan

D·ª± √°n implement Convolutional Autoencoder v·ªõi CUDA ƒë·ªÉ:
1. Extract features t·ª´ CIFAR-10 dataset (60K ·∫£nh 32√ó32√ó3)
2. Train SVM classifier tr√™n features ƒë√£ extract
3. T·ªëi ∆∞u h√≥a v·ªõi GPU ƒë·ªÉ ƒë·∫°t speedup >20√ó

### Ki·∫øn tr√∫c Autoencoder

**Encoder:**
```
Input (32√ó32√ó3) ‚Üí Conv2D(256) + ReLU ‚Üí MaxPool ‚Üí (16√ó16√ó256)
                ‚Üí Conv2D(128) + ReLU ‚Üí MaxPool ‚Üí (8√ó8√ó128) = Latent (8192-D)
```

**Decoder:**
```
Latent (8√ó8√ó128) ‚Üí Conv2D(128) + ReLU ‚Üí Upsample ‚Üí (16√ó16√ó128)
                 ‚Üí Conv2D(256) + ReLU ‚Üí Upsample ‚Üí (32√ó32√ó256)
                 ‚Üí Conv2D(3) ‚Üí Output (32√ó32√ó3)
```

---

## üìÇ C·∫•u tr√∫c th∆∞ m·ª•c

```
Project/
‚îú‚îÄ‚îÄ README.md                    # ‚Üê File n√†y
‚îú‚îÄ‚îÄ notebook.ipynb               # ‚Üê Notebook duy nh·∫•t ƒë·ªÉ ch·∫°y m·ªçi th·ª©
‚îú‚îÄ‚îÄ run_pipeline.py              # Python wrapper
‚îÇ
‚îú‚îÄ‚îÄ cifar-10-batches-bin/       # CIFAR-10 dataset (binary)
‚îÇ   ‚îú‚îÄ‚îÄ data_batch_1.bin
‚îÇ   ‚îú‚îÄ‚îÄ data_batch_2.bin
‚îÇ   ‚îú‚îÄ‚îÄ data_batch_3.bin
‚îÇ   ‚îú‚îÄ‚îÄ data_batch_4.bin
‚îÇ   ‚îú‚îÄ‚îÄ data_batch_5.bin
‚îÇ   ‚îî‚îÄ‚îÄ test_batch.bin
‚îÇ
‚îú‚îÄ‚îÄ include/                     # Header files
‚îÇ   ‚îú‚îÄ‚îÄ cifar10_loader.h
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_cpu.h
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_gpu.h
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_gpu_optimized.h
‚îÇ   ‚îî‚îÄ‚îÄ svm_classifier.h
‚îÇ
‚îú‚îÄ‚îÄ src/                        # Source code
‚îÇ   ‚îú‚îÄ‚îÄ cifar10_loader.cpp
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_cpu.cpp
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_gpu.cu
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_gpu_optimized.cu
‚îÇ   ‚îú‚îÄ‚îÄ svm_classifier.cpp
‚îÇ   ‚îú‚îÄ‚îÄ main_phase1.cpp
‚îÇ   ‚îú‚îÄ‚îÄ main_phase2.cpp
‚îÇ   ‚îú‚îÄ‚îÄ main_phase3.cpp
‚îÇ   ‚îî‚îÄ‚îÄ main_phase4.cpp
‚îÇ
‚îú‚îÄ‚îÄ cuda/                       # CUDA kernels
‚îÇ   ‚îú‚îÄ‚îÄ gpu_kernels.h
‚îÇ   ‚îú‚îÄ‚îÄ gpu_kernels.cu
‚îÇ   ‚îú‚îÄ‚îÄ gpu_kernels_optimized.h
‚îÇ   ‚îî‚îÄ‚îÄ gpu_kernels_optimized.cu
‚îÇ
‚îú‚îÄ‚îÄ Makefile                    # Build system
‚îú‚îÄ‚îÄ CMakeLists.txt             # Alternative build (CMake)
‚îÇ
‚îú‚îÄ‚îÄ build/                      # Compiled binaries (generated)
‚îÇ   ‚îú‚îÄ‚îÄ phase1                 # CPU baseline
‚îÇ   ‚îú‚îÄ‚îÄ phase2                 # Naive GPU
‚îÇ   ‚îú‚îÄ‚îÄ phase3                 # Optimized GPU
‚îÇ   ‚îî‚îÄ‚îÄ phase4                 # Full pipeline with SVM
‚îÇ
‚îú‚îÄ‚îÄ weights/                    # Model weights (generated)
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_cpu.weights
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_gpu.weights
‚îÇ   ‚îî‚îÄ‚îÄ autoencoder_gpu_optimized.weights
‚îÇ
‚îî‚îÄ‚îÄ third_party/               # External libraries
    ‚îî‚îÄ‚îÄ libsvm/                # SVM library
```

---

## üöÄ Quick Start

### 1. Setup m√¥i tr∆∞·ªùng

#### Y√™u c·∫ßu:
- **CUDA:** >= 11.0 (check: `nvcc --version`)
- **GPU:** NVIDIA v·ªõi compute capability >= 7.5
- **Compiler:** g++ >= 7.0
- **Python:** >= 3.7 (n·∫øu d√πng notebook)

#### C√†i ƒë·∫∑t dependencies:
```bash
# Clone v√† build LIBSVM
cd third_party
git clone https://github.com/cjlin1/libsvm.git
cd libsvm
make
cd ../..
```

### 2. Download CIFAR-10 dataset

```bash
# Download
wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz

# Extract
tar -xzf cifar-10-binary.tar.gz

# ƒê·∫£m b·∫£o c√≥ th∆∞ m·ª•c cifar-10-batches-bin/
```

### 3. Compile project

```bash
# Compile t·∫•t c·∫£ phases
make all

# Ho·∫∑c compile t·ª´ng phase
make phase1  # CPU baseline
make phase2  # Naive GPU
make phase3  # Optimized GPU
make phase4  # Full pipeline with SVM
```

**L∆∞u √Ω:** ƒêi·ªÅu ch·ªânh CUDA architecture trong Makefile n·∫øu c·∫ßn:
```makefile
CUDA_ARCH = -arch=sm_75  # RTX 2080, T4
# sm_80: A100
# sm_86: RTX 3090
```

### 4. Run

```bash
# Phase 1: CPU Baseline
./build/phase1

# Phase 2: Naive GPU
./build/phase2

# Phase 3: Optimized GPU  
./build/phase3

# Phase 4: SVM Classification
./build/phase4
```

---

## üìä 4 Phases Implementation

### Phase 1: CPU Baseline (Sanity Check)
- **File:** `src/autoencoder_cpu.cu`, `src/main_phase1.cu`
- **M·ª•c ƒë√≠ch:** Sanity check + Benchmark baseline
- **Configuration:**
  - **Training:** 50,000 images (full dataset), 1 epoch
  - **Purpose:** 
    - ‚úÖ Sanity check: ƒê·∫£m b·∫£o code kh√¥ng crash, t√≠nh to√°n ƒë√∫ng (no NaN/Inf)
    - ‚úÖ Benchmarking: ƒêo th·ªùi gian ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng full training (20 epochs)
  - **Fast test mode:** Uncomment d√≤ng code ƒë·ªÉ ch·ªâ d√πng 300 ·∫£nh test nhanh
- **Features:** 
  - Pure C++ implementation
  - Nested loops cho convolution
  - Simplified backward pass
- **Expected time:** 
  - Full dataset (50,000 images, 1 epoch): ~90 minutes
  - Fast test (300 images, 1 epoch): ~30 seconds

### Phase 2: Naive GPU (Full Training)
- **Files:** `src/autoencoder_gpu.cu`, `cuda/gpu_kernels.cu`
- **M·ª•c ƒë√≠ch:** GPU implementation ƒë∆°n gi·∫£n
- **Configuration:**
  - **Training:** 50,000 images, 20 epochs (FULL)
  - **Batch size:** 64
- **Features:**
  - Basic CUDA kernels
  - Sequential kernel launches
  - Standard memory transfers
- **Expected speedup:** 6-10√ó vs CPU

### Phase 3: Optimized GPU (Full Training)
- **Files:** `src/autoencoder_gpu_optimized.cu`, `cuda/gpu_kernels_optimized.cu`
- **Configuration:**
  - **Training:** 50,000 images, 20 epochs (FULL)
  - **Batch size:** 128 (t·ªëi ∆∞u GPU utilization)
- **Optimizations:**
  - ‚úÖ **Kernel fusion:** Conv2D + ReLU trong 1 kernel
  - ‚úÖ **Pinned memory:** Faster CPU‚ÜîGPU transfers
  - ‚úÖ **Async transfers:** Overlap computation
  - ‚úÖ **Larger batch size:** Better GPU utilization
  - üîß **Shared memory tiling:** Template provided (cho future)
- **Expected speedup:** 15-25√ó vs CPU, ~2√ó vs Naive GPU

### Phase 4: SVM Classification (Full Pipeline)
- **Files:** `src/main_phase4.cu`, `src/svm_classifier.cu`
- **Configuration:**
  - **Training:** 50,000 images (full dataset)
  - **Test:** 10,000 images
- **Pipeline:**
  1. Load trained autoencoder weights
  2. Extract features (8192-D) t·ª´ 60K images
  3. Train SVM v·ªõi RBF kernel
  4. Evaluate tr√™n test set
- **Expected accuracy:** 60-65%

---

## üêç Ch·∫°y t·ª´ Python/Jupyter

### Setup Python wrapper

```python
from run_pipeline import CIFARAutoencoderPipeline

pipeline = CIFARAutoencoderPipeline()

# Check m√¥i tr∆∞·ªùng
pipeline.check_setup()

# Compile
pipeline.compile_all()

# Run phases
result1 = pipeline.run_phase1_cpu()
result2 = pipeline.run_phase2_gpu()
result3 = pipeline.run_phase3_optimized()
result4 = pipeline.run_phase4_svm()

# Compare
print(f"CPU: {result1['time']:.2f}s")
print(f"Optimized GPU: {result3['time']:.2f}s")
print(f"Speedup: {result1['time'] / result3['time']:.2f}√ó")
```

### Command line

```bash
python run_pipeline.py check     # Ki·ªÉm tra setup
python run_pipeline.py compile   # Compile all
python run_pipeline.py phase3    # Run Phase 3
python run_pipeline.py all       # Run to√†n b·ªô pipeline
python run_pipeline.py profile   # Profile v·ªõi nsys
```

### Jupyter Notebook

M·ªü file `notebook.ipynb` - t√≠ch h·ª£p ƒë·∫ßy ƒë·ªß:
- Setup & compilation
- Run t·ª´ng phase
- Visualize k·∫øt qu·∫£
- So s√°nh performance
- Report template

---

## üåê Google Colab

### Setup tr√™n Colab

```python
# 1. Check GPU
!nvidia-smi

# 2. Clone project
!git clone <your-repo-url>
%cd Project

# 3. Install LIBSVM
!cd third_party && git clone https://github.com/cjlin1/libsvm.git && cd libsvm && make

# 4. Download CIFAR-10
!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz
!tar -xzf cifar-10-binary.tar.gz

# 5. Compile
!make all

# 6. Run
!./build/phase3
```

### S·ª≠ d·ª•ng notebook.ipynb tr√™n Colab

1. Upload `notebook.ipynb` l√™n Google Drive
2. M·ªü b·∫±ng Google Colab
3. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)
4. Run t·ª´ng cell

---

## üéØ Performance Targets

| Metric | Target | 
|--------|--------|
| Training time | < 10 ph√∫t (600s) |
| Feature extraction | < 20s cho 60K images |
| Speedup (Phase 3 vs CPU) | > 20√ó |
| Test accuracy | 60-65% |

---

## üéì K·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a (Phase 3)

### 1. Kernel Fusion
**Problem:** M·ªói kernel c√≥ overhead (launch + sync)
**Solution:** Merge Conv2D + ReLU th√†nh 1 kernel

```cpp
// Before: 2 kernels
conv2d_kernel<<<grid, block>>>(input, temp, weights, bias);
relu_kernel<<<grid, block>>>(temp, output);

// After: 1 fused kernel  
conv2d_relu_kernel<<<grid, block>>>(input, output, weights, bias);
```

**Benefit:** Gi·∫£m 50% memory traffic, 15-20% faster

### 2. Pinned Memory
**Problem:** Pageable memory ‚Üí slow CPU‚ÜîGPU transfer
**Solution:** Pinned (page-locked) memory

```cpp
// Before
float* h_data = new float[size];
cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);

// After
float* h_pinned;
cudaMallocHost(&h_pinned, size);  // Pinned memory
cudaMemcpyAsync(d_data, h_pinned, size, cudaMemcpyHostToDevice);
```

**Benefit:** 2√ó faster transfer

### 3. Async Transfers
**Problem:** Transfer blocking computation
**Solution:** Overlap transfer & computation

```cpp
cudaMemcpyAsync(d_input, h_input, size, ..., stream);
kernel<<<grid, block, 0, stream>>>(...);  // Run while transferring
```

**Benefit:** Hide transfer latency

### 4. Larger Batch Size
**Problem:** Small batches ‚Üí low GPU utilization
**Solution:** Increase batch size 64 ‚Üí 128

**Benefit:** Better occupancy, 10-15% faster

---

## üîß Troubleshooting

### Compilation errors

**CUDA not found:**
```bash
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
```

**Wrong GPU architecture:**
```bash
# Check GPU compute capability
nvidia-smi --query-gpu=compute_cap --format=csv

# Update Makefile
CUDA_ARCH = -arch=sm_XX  # Replace XX
```

### Runtime errors

**Out of memory:**
- Gi·∫£m batch size trong source code
- Phase 2/3: S·ª≠a `int batch_size = 64` ‚Üí `32`

**LIBSVM not found:**
```bash
cd third_party
git clone https://github.com/cjlin1/libsvm.git
cd libsvm && make
```

**CIFAR-10 dataset not found:**
```bash
wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz
tar -xzf cifar-10-binary.tar.gz
```

---

## üìà Profiling

### NVIDIA Nsight Systems

```bash
# Profile Phase 3
nsys profile --output=phase3_profile ./build/phase3

# View results
nsys-ui phase3_profile.nsys-rep
```

### NVIDIA Nsight Compute

```bash
# Detailed kernel analysis
ncu --set full --export phase3_kernel ./build/phase3

# View
ncu-ui phase3_kernel.ncu-rep
```

---

## üìù Report Template

### N·ªôi dung b√°o c√°o

1. **Gi·ªõi thi·ªáu**
   - M√¥ t·∫£ ƒë·ªÅ b√†i
   - Ki·∫øn tr√∫c Autoencoder
   - M·ª•c ti√™u performance

2. **Implementation**
   - Phase 1: CPU Baseline
   - Phase 2: Naive GPU
   - Phase 3: Optimized GPU (chi ti·∫øt optimizations)
   - Phase 4: SVM Classification

3. **Results**
   - B·∫£ng so s√°nh timing
   - Bi·ªÉu ƒë·ªì speedup
   - Confusion matrix
   - Per-class accuracy

4. **Analysis**
   - Profiling results (nsys/ncu)
   - Bottleneck analysis
   - Optimization effectiveness

5. **Conclusion**
   - Th√†nh t·ª±u ƒë·∫°t ƒë∆∞·ª£c
   - H·∫°n ch·∫ø
   - Future work

### Video Demo (15-20 ph√∫t)

1. Gi·ªõi thi·ªáu ƒë·ªÅ b√†i (2 ph√∫t)
2. Demo compilation & execution (3 ph√∫t)
3. Gi·∫£i th√≠ch code ch√≠nh (5 ph√∫t)
4. So s√°nh k·∫øt qu·∫£ (3 ph√∫t)
5. Ph√¢n t√≠ch optimizations (4 ph√∫t)
6. K·∫øt lu·∫≠n (1 ph√∫t)

---

## üìñ API Reference

### CIFAR10Loader
```cpp
Cifar10Loader loader("cifar-10-batches-bin");
loader.load();  // Load t·∫•t c·∫£ data

auto& train_images = loader.get_train_images();  // 50000 √ó 3072
auto& test_images = loader.get_test_images();    // 10000 √ó 3072
```

### AutoencoderCPU
```cpp
AutoencoderCPU model;
model.train(train_images, num_images, batch_size, epochs, lr);
model.extract_features(images, num_images, features);  // ‚Üí 8192-D
model.save_weights("path/to/weights");
```

### AutoencoderGPU / AutoencoderGPUOptimized
```cpp
AutoencoderGPU model;  // ho·∫∑c AutoencoderGPUOptimized
model.train(train_images, num_images, batch_size, epochs, lr);
model.extract_features(images, num_images, features);
```

### SVMClassifier
```cpp
SVMClassifier svm;
svm.train(train_features, train_labels, num_train);
float accuracy = svm.predict(test_features, test_labels, num_test);
svm.save_model("svm_model.txt");
```

---

## ü§ù Contributing

N·∫øu mu·ªën m·ªü r·ªông project:

1. **Phase 3 improvements:**
   - Implement shared memory tiling (template ƒë√£ c√≥)
   - Multi-stream execution
   - Mixed precision (FP16)

2. **Architecture variants:**
   - Try deeper networks
   - ResNet-style skip connections
   - Different latent dimensions

3. **Other optimizations:**
   - cuDNN library integration
   - Dynamic batch sizing
   - Gradient checkpointing

---

## üìö References

- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)
- [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)
- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems)

---

## üìÑ License

Educational project for CSC14120 - Parallel Programming Course

---

**Good luck! üöÄ**
